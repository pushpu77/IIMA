{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed23713",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTANT POINTS TO REMEMBER WHILE RUNNING THIS CODE\n",
    "# 1. Specify the folder path where all the files are present\n",
    "# 2. Specify the starting year and the window size \n",
    "# 3. The code is written sector wise -- for this we will need (for_puspendra.sas7bdat) to be kept in the pwd\n",
    "# 4. Before running code make sure all the files in your folder are renamed as AccessionNumber.txt\n",
    "# 5. If you want to use the words from particular financial books -- specify the texts book and their page number \n",
    "# 6. If you want to use the words from particular financial books -- tune the variable to onlyWords to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c282d1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries to load\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import datetime\n",
    "import time\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c4032b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "#load library\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim import corpora, models\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import gensim\n",
    "from gensim import models\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91326722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(400)\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8d8dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting all the sectors\n",
    "st = pd.read_sas('for_puspendra.sas7bdat')\n",
    "st['GSECTOR'] = st['GSECTOR'].astype(str)\n",
    "st['GSECTOR'] =st['GSECTOR'].str.strip(\"b'\")\n",
    "st['accession'] = st['accession'].astype(str)\n",
    "st['accession'] =st['accession'].str.strip(\"b'\")\n",
    "st['year'] = st['accession'].str.split('-').str[1]\n",
    "st['year'] = st['year'].astype(int)\n",
    "gsectors=st['GSECTOR'].unique().tolist()\n",
    "gsectors=gsectors[0:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9441ee9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this to download libraries if running code for the first time\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf2f475",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to preprocess the textual data\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    # Remove auto-generated electronic filings\n",
    "    text = re.sub(r'<TYPE>.*?</TYPE>', '', text, flags=re.DOTALL)\n",
    "    \n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # Remove XBRL data\n",
    "    text = re.sub(r'<XBRL>.*?</XBRL>', '', text, flags=re.DOTALL)\n",
    "\n",
    "    # Remove outlier text after </TEXT> tag\n",
    "    text = re.sub(r'</TEXT>.*', '', text, flags=re.DOTALL)\n",
    "\n",
    "    # Remove paragraph tags\n",
    "    text = re.sub(r'<p>', '', text)\n",
    "\n",
    "    # Remove punctuation marks except periods and commas\n",
    "    text = re.sub(r'[^\\w\\s.,]', '', text)\n",
    "\n",
    "    # Remove text between <TABLE> and </TABLE> tags\n",
    "    text = re.sub(r'<TABLE>.*?</TABLE>', '', text, flags=re.DOTALL)\n",
    "\n",
    "    # Remove paragraphs with less than 50 words\n",
    "    paragraphs = text.split('\\n')\n",
    "    paragraphs = [p for p in paragraphs if len(p.split()) >= 20]\n",
    "    text = '\\n'.join(paragraphs)\n",
    "\n",
    "    # Clean up extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "\n",
    "    text = text.lower()\n",
    "    # Remove punctuations\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove dates and years\n",
    "    text = re.sub(r'\\b\\d{1,2}/\\d{1,2}/\\d{2,4}\\b', '', text)  # Removing dates in the format MM/DD/YYYY or MM/DD/YY\n",
    "    text = re.sub(r'\\b\\d{1,2}-\\d{1,2}-\\d{2,4}\\b', '', text)  # Removing dates in the format MM-DD-YYYY or MM-DD-YY\n",
    "    text = re.sub(r'\\b\\d{1,2}\\s+\\w+\\s+\\d{2,4}\\b', '', text)  # Removing dates in the format DD Month YYYY or DD Month YY\n",
    "    text = re.sub(r'\\b\\d{4}\\b', '', text)  # Removing standalone years (4 digits)\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    polite_words = [\n",
    "        'please', 'thank', 'thank you', 'excuse', 'pardon', 'may I', ' please', 'appreciate', \n",
    "        \"grateful\", ' don\\'t mind', 'sorry', 'inconvenience', ' welcome', 'apologies', 'kindly',\n",
    "        ' mind', ' bother', 'allow ', 'possible', 'trouble','this'\n",
    "    ]\n",
    "    stop_words.update(polite_words)\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    text = ' '.join(tokens)\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5070a65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create bigrams of word from the textual data\n",
    "from gensim.utils import simple_preprocess\n",
    "def preprocess_documents(documents):\n",
    "    processed_docs = [simple_preprocess(doc, deacc=True, min_len=2, max_len=15) for doc in documents]\n",
    "#     processed_docs = [word_tokenize(doc) for doc in documents]\n",
    "    processed_docs = [list(map(str.encode, doc)) for doc in processed_docs]\n",
    "    \n",
    "    # Create bigrams\n",
    "    bigram_mod = []\n",
    "    for doc in processed_docs:\n",
    "        bigrams = list(ngrams(doc, 2))\n",
    "        bigram_mod.append([' '.join(map(bytes.decode, bigram)) for bigram in bigrams])\n",
    "\n",
    "    return bigram_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7582ec01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2  # Import the PyPDF2 library for PDF operations\n",
    "\n",
    "filename = 'FinancialDictionary.pdf'  # Specify the PDF file name\n",
    "\n",
    "pdfFileObj = open(filename, 'rb')  # Open the PDF file in read-binary mode\n",
    "pdfReader = PyPDF2.PdfReader(pdfFileObj)  # Create a PdfReader object to read the PDF\n",
    "num_pages = len(pdfReader.pages)  # Get the total number of pages in the PDF\n",
    "\n",
    "start_page = 1  # Specify the starting page\n",
    "end_page = 5    # Specify the ending page\n",
    "\n",
    "count = start_page - 1  # Initialize the count variable to the starting page index\n",
    "text = \"\"  # Initialize an empty string to store the extracted text\n",
    "\n",
    "# Iterate over the desired pages and extract text from each page\n",
    "while count < num_pages and count < end_page:\n",
    "    pageObj = pdfReader.pages[count]  # Get the page object for the current page\n",
    "    count += 1  # Increment the count variable\n",
    "    text += pageObj.extract_text()  # Extract text from the page and append it to the 'text' string\n",
    "\n",
    "# Check if text is empty after processing desired pages\n",
    "if text != \"\":\n",
    "    text = text  # If text is not empty, assign it to the 'text' variable\n",
    "else:\n",
    "    # If text is empty, extract text using textract library from the specified URL\n",
    "    text = textract.process('http://bit.ly/epo_keyword_extraction_document', method='tesseract', language='eng')\n",
    "\n",
    "text = text.encode('ascii', 'ignore').lower()  # Convert the text to lowercase and remove non-ASCII characters\n",
    "\n",
    "uniqueWords = [] \n",
    "for i in keywords:\n",
    "      if not i in uniqueWords:\n",
    "            uniqueWords.append(i);\n",
    "                \n",
    "uniqueWords\n",
    "wordss=uniqueWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac44808",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "folder_path = 'ksen'  # Folder path containing the 10K files\n",
    "csv_file = 'cleaned_text_10K.csv'  # CSV file path to store the cleaned text\n",
    "num_of_topics = 30 #Specify the number of topics\n",
    "yr = 5 #Specify the starting year\n",
    "window = 3 #Specify the window size\n",
    "onlyWords=0 # 1 for yes and 0 for no\n",
    "# Iterate over the desired years\n",
    "for x in range(yr, yr + 22 - 5 - window + 1, 1):\n",
    "#     if x == 15:\n",
    "#         break\n",
    "        \n",
    "    # Filter the DataFrame based on the year and sector\n",
    "    if window == 1:\n",
    "        filtered_df = st[(st['year'].isin([x]))]\n",
    "    if window == 3:\n",
    "        filtered_df = st[(st['year'].isin([x, x + 1, x + 2]))]\n",
    "    if window == 5:\n",
    "        filtered_df = st[(st['year'].isin([x, x + 1, x + 2, x + 3, x + 4]))]\n",
    "\n",
    "    # Get the accession numbers from the filtered DataFrame\n",
    "    accession_numbers = filtered_df['accession'].tolist()\n",
    "\n",
    "    # Open the CSV file for writing\n",
    "    with open(csv_file, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Accession Number', 'Cleaned Text'])\n",
    "\n",
    "        # Iterate over the accession numbers\n",
    "        for accession_number in accession_numbers:\n",
    "            filename = accession_number + '.txt'\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "            if os.path.isfile(file_path):  # Check if the path is a file\n",
    "                with open(file_path, 'r') as file:\n",
    "                    content = file.read()\n",
    "\n",
    "                # Preprocess and clean the text\n",
    "                cleaned_text = preprocess_text(content)\n",
    "\n",
    "                if not cleaned_text:\n",
    "                    cleaned_text = content\n",
    "\n",
    "                # Write the cleaned text and accession number to the CSV file\n",
    "                writer.writerow([accession_number, cleaned_text])\n",
    "                    \n",
    "        # Read the cleaned text DataFrame\n",
    "        df = pd.read_csv('cleaned_text_10K.csv')\n",
    "        df = df.reset_index(drop=True)\n",
    "        \n",
    "        if(onlyWords):\n",
    "            df['Cleaned Text']=df['Cleaned Text'].str.lower().str.split()\n",
    "            df['Cleaned Text'] = df['Cleaned Text'].apply(lambda words: [word for word in words if word in wordss])\n",
    "            df['Cleaned Text'] = df['Cleaned Text'].apply(lambda x: ' '.join(x))\n",
    "        \n",
    "        # Convert the 'Cleaned Text' column to a list\n",
    "        flat_list = df['Cleaned Text'].tolist()\n",
    "        \n",
    "        # Preprocess the documents\n",
    "        processed_docs = preprocess_documents(flat_list)\n",
    "        \n",
    "        # Create a Gensim Dictionary object\n",
    "        dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "\n",
    "        # Remove extreme words (very common and very rare) from the dictionary\n",
    "        dictionary.filter_extremes(no_below=2, no_above=0.01)\n",
    "        \n",
    "        # Check if the dictionary is empty after filtering\n",
    "        if len(dictionary) == 0:\n",
    "            dictionary = gensim.corpora.Dictionary(processed_docs)  \n",
    "\n",
    "        # Create bag-of-word (BoW) models for each document\n",
    "        bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "    \n",
    "        # Create TF-IDF model from the BoW corpus\n",
    "        tfidf = models.TfidfModel(bow_corpus)\n",
    "        corpus_tfidf = tfidf[bow_corpus]\n",
    "\n",
    "        # Apply LDA topic modeling using TF-IDF\n",
    "        lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, \n",
    "                                                     num_topics=num_of_topics, \n",
    "                                                     id2word=dictionary, \n",
    "                                                     passes=10, \n",
    "                                                     workers=4)\n",
    "        \n",
    "        # Define the output filename for the topics\n",
    "        if(window==3):\n",
    "            filenameee = f\"topics_{int(x) + 2000}_{int(x + 1) + 2000}_{int(x + 2) + 2000}.csv\"\n",
    "\n",
    "        if(window==5):\n",
    "            filenameee = f\"topics_{int(x) + 2000}_{int(x + 1) + 2000}_{int(x + 2) + 2000}_{int(x + 3) + 2000}_{int(x + 4) + 2000}.csv\"\n",
    "        \n",
    "        if(window==1):\n",
    "            filenameee = f\"topics_{int(x) + 2000}.csv\"\n",
    "\n",
    "        # Write the topics to a CSV file\n",
    "        with open(filenameee, 'w', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(['Words'])\n",
    "\n",
    "            for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "                topic_parts = topic.split('\"')[1::2]  # Extract the words within double quotes\n",
    "                words = ', '.join(topic_parts)\n",
    "                writer.writerow([words])\n",
    "\n",
    "        # Create a DataFrame to store the document-topic distribution\n",
    "        topic_columns = [\"topic\" + str(i) for i in range(1, num_of_topics + 1)]\n",
    "        documents_lda_tfidf_topics = pd.DataFrame(columns=topic_columns)\n",
    "\n",
    "        # Iterate over the documents\n",
    "        for i in range(len(bow_corpus)):\n",
    "            if i % 500 == 0:\n",
    "                print(i)\n",
    "            \n",
    "            # Initialize the topic distribution for the document\n",
    "            documents_lda_tfidf_topics.loc[i] = np.zeros(num_of_topics)\n",
    "            \n",
    "            # Get the topic distribution for the document\n",
    "            output_tfidf = lda_model_tfidf.get_document_topics(bow_corpus[i])\n",
    "            \n",
    "            # Update the document-topic distribution DataFrame\n",
    "            for k in range(len(output_tfidf)):\n",
    "                a = output_tfidf[k][0]\n",
    "                b = output_tfidf[k][1]\n",
    "                documents_lda_tfidf_topics.iloc[i, a] = b\n",
    "\n",
    "        # Filter the DataFrame based on the accession numbers for the next year\n",
    "        if(window==3):\n",
    "            if (x + 2) < 10 and x > 5:\n",
    "                filtered_df = df[df['Accession Number'].str.contains('-0' + str(x + 2) + '-')]\n",
    "            elif x == 5:   \n",
    "                filtered_df = df[df['Accession Number'].str.contains('-0' + str(x + 2)) |\n",
    "                              df['Accession Number'].str.contains('-0' + str(x + 1)) |\n",
    "                              df['Accession Number'].str.contains('-0' + str(x))]\n",
    "            else:\n",
    "                filtered_df = df[df['Accession Number'].str.contains('-' + str(x + 2) + '-')]\n",
    "\n",
    "        if(window==5):\n",
    "            if (x + 4) >= 10:\n",
    "                filtered_df = df[df['Accession Number'].str.contains('-' + str(x + 4) + '-')]\n",
    "            elif x == 5:   \n",
    "                filtered_df = df[df['Accession Number'].str.contains('-0' + str(x + 2)) |\n",
    "                              df['Accession Number'].str.contains('-0' + str(x + 1)) |\n",
    "                              df['Accession Number'].str.contains('-0' + str(x))|\n",
    "                              df['Accession Number'].str.contains('-0' + str(x + 3))|\n",
    "                              df['Accession Number'].str.contains('-0' + str(x + 4))]\n",
    "        \n",
    "        if(window==1):\n",
    "            if(x>9):\n",
    "                filtered_df = df[df['Accession Number'].str.contains('-' + str(x) + '-')]\n",
    "            else:\n",
    "                filtered_df = df[df['Accession Number'].str.contains('-0' + str(x) + '-')]\n",
    "\n",
    "\n",
    "\n",
    "        # Concatenate the filtered DataFrame with the document-topic distribution DataFrame\n",
    "        documents_lda_tfidf_topics = pd.concat([filtered_df[['Accession Number']], documents_lda_tfidf_topics], axis=1)\n",
    "        \n",
    "        # Drop any rows with missing values\n",
    "        documents_lda_tfidf_topics = documents_lda_tfidf_topics.dropna()\n",
    "        \n",
    "        # Reset the index of the DataFrame\n",
    "        documents_lda_tfidf_topics = documents_lda_tfidf_topics.reset_index(drop=True)\n",
    "        \n",
    "        # Define the output filename for the document-topic distribution\n",
    "        if(window==1):\n",
    "            filenamee = f\"TopicModellingText_TFIDF_{int(x) + 2000}.csv\"\n",
    "        if(window==3):  \n",
    "            filenamee = f\"TopicModellingText_TFIDF_{int(x + 2) + 2000}.csv\"\n",
    "        if(window==5):  \n",
    "            filenamee = f\"TopicModellingText_TFIDF_{int(x + 4) + 2000}.csv\"  \n",
    "\n",
    "        # Save the DataFrame to a CSV file\n",
    "        documents_lda_tfidf_topics.to_csv(filenamee)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
